# -*- coding: utf-8 -*-
"""CC25_CF019_Klasifikasi_Mata_Terbuka_dan_Mata_Tertutup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4kltzC0sJYPefQ2_5UVeNGeO7ESQ4e_

# Proyek Klasifikasi Gambar: Drowsiness Detection Dataset

## Import Semua Packages/Library yang Digunakan
"""

import os
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm as tq

# Libraries untuk pemrosesan data gambar
import cv2
from PIL import Image

# Libraries untuk pembangunan model
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

"""## Data Preparation

### Data Loading
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#!/bin/bash
!kaggle datasets download -d hazemfahmy/openned-closed-eyes

!unzip openned-closed-eyes.zip

dataset_path = "ImprovementSet/ImprovementSet/ImprovementSet"

# Membuat kamus yang menyimpan daftar gambar untuk setiap kelas
drowsiness_image = {}

# Mengambil daftar kelas dari folder dataset
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)

    # Hanya folder yang diproses
    if os.path.isdir(class_path):
        image_list = os.listdir(class_path)

        # Hanya gambar yang diproses (hindari folder kosong)
        if len(image_list) >= 5:
            drowsiness_image[class_name] = image_list

# Menampilkan secara acak 5 gambar dari setiap kelas (2 kelas)
num_classes = min(2, len(drowsiness_image))  # Ambil 2 kelas
fig, axs = plt.subplots(num_classes, 5, figsize=(15, 15))

# Looping untuk menampilkan gambar dari setiap kelas
for i, (class_name, images) in enumerate(drowsiness_image.items()):
    if i >= 2:  # Batasi hanya 2 kelas
        break

    # 5 gambar secara acak dari kelas
    selected_images = np.random.choice(images, 5, replace=False)

    for j, image_name in enumerate(selected_images):
        img_path = os.path.join(dataset_path, class_name, image_name)

        try:
            # Buka gambar dan konversi ke grayscale
            img = Image.open(img_path).convert("RGB")

            # Tampilkan gambar
            axs[i, j].imshow(img, cmap='gray')
            axs[i, j].set_title(class_name, fontsize=10)
            axs[i, j].axis("off")  # Hilangkan axis/ticks
        except Exception as e:
            print(f"Error membuka gambar {img_path}: {e}")

# Rapikan tampilan
plt.tight_layout()
plt.show()

# Iterasi tiap folder kelas dan hitung jumlah file di dalamnya
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        total_files = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])
        print(f"{class_name}: {total_files} file")

def print_images_resolution(directory):
    unique_sizes = set()
    total_images = 0

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)
        image_files = os.listdir(subdir_path)
        num_images = len(image_files)
        print(f"{subdir}: {num_images}")
        total_images += num_images

        for img_file in image_files:
            img_path = os.path.join(subdir_path, img_file)
            with Image.open(img_path) as img:
                unique_sizes.add(img.size)

        for size in unique_sizes:
            print(f"- {size}")
        print("---------------")

    print(f"\nTotal: {total_images}")
print_images_resolution(dataset_path)

# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data
file_name = []
labels = []
full_path = []

# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe
for path, subdirs, files in os.walk(dataset_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path, 'file_name':file_name, "labels":labels})

# Plot distribusi gambar di setiap kelas
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

"""### Data Preprocessing

#### Split Dataset
"""

# Path asal
base_path = "ImprovementSet/ImprovementSet/ImprovementSet"
output_base = "dataset_split"  # folder output
classes = ['Closed', 'Opened']
test_size = 0.2  # 20% test

# Membuat folder output
for split in ['train', 'test']:
    for class_name in classes:
        os.makedirs(os.path.join(output_base, split, class_name), exist_ok=True)

# Memproses tiap kelas
for class_name in classes:
    class_path = os.path.join(base_path, class_name)
    images = os.listdir(class_path)

    # Bagi train dan test
    train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)

    # Salin ke folder train
    for img in train_images:
        src = os.path.join(class_path, img)
        dst = os.path.join(output_base, 'train', class_name, img)
        shutil.copy(src, dst)

    # Salin ke folder test
    for img in test_images:
        src = os.path.join(class_path, img)
        dst = os.path.join(output_base, 'test', class_name, img)
        shutil.copy(src, dst)

print("Dataset berhasil dipisahkan ke train/test")

train_dir = "dataset_split/train"
test_dir = "dataset_split/test"

# Data augmentation dan normalisasi (training + validation split)
train_val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Generator untuk training (80%)
train_generator = train_val_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary',
    color_mode='rgb',
    shuffle=True,
    subset='training'
)

# Generator untuk validation (20%)
validation_generator = train_val_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary',
    color_mode='rgb',
    shuffle=False,
    subset='validation'
)

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=test_dir,
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary',
    color_mode='rgb',
    shuffle=False
)

"""## Modelling"""

# Inisialisasi model Sequential
model = Sequential()

# Tambahkan layer konvolusi pertama
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))

# Tambahkan layer max pooling pertama
model.add(MaxPooling2D((2, 2)))

# Tambahkan dropout setelah pooling pertama
model.add(Dropout(0.25))

# Tambahkan layer konvolusi kedua
model.add(Conv2D(64, (3, 3), activation='relu'))

# Tambahkan layer max pooling kedua
model.add(MaxPooling2D((2, 2)))

# Tambahkan dropout setelah pooling kedua
model.add(Dropout(0.25))

# Tambahkan layer konvolusi ketiga
model.add(Conv2D(128, (3, 3), activation='relu'))

# Flatten layer untuk meratakan output
model.add(Flatten())

# Tambahkan dropout sebelum dense layer
model.add(Dropout(0.5))

# Tambahkan layer dense dengan 128 neuron
model.add(Dense(128, activation='relu'))

# Tambahkan dropout setelah dense layer
model.add(Dropout(0.5))

# Tambahkan output layer dengan fungsi aktivasi softmax
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)

history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=30,
    callbacks=[early_stop, reduce_lr]
)

# Menggunakan MobileNetV2 tanpa lapisan akhir (include_top=False)
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Membekukan semua layer pada model pretrained agar tidak ikut di-train
base_model.trainable = False

# Bangun model baru dengan tambahan lapisan klasifikasi
x = base_model.output
x = GlobalAveragePooling2D()(x)  # Mengubah 4D tensor menjadi 2D
x = Dropout(0.5)(x)               # Dropout untuk menghindari overfitting
x = Dense(128, activation='relu')(x)  # Lapisan fully connected
output = Dense(1, activation='sigmoid')(x)  # Output binary classification (mata terbuka/tertutup)

# Buat model final
model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=30,
    callbacks=[early_stop, reduce_lr]
)

"""### Fine Tuning MobileNetV2"""

base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Compile ulang dengan learning rate yang lebih kecil
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Learning rate lebih kecil
              loss='binary_crossentropy',
              metrics=['accuracy'])

history_finetune = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=20,
    callbacks=[early_stop, reduce_lr]
)

"""## Evaluasi dan Visualisasi"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validaion Loss')
plt.show()

# Hasil finetune MobileNetV2
acc = history_finetune.history['accuracy']
val_acc = history_finetune.history['val_accuracy']
loss = history_finetune.history['loss']
val_loss = history_finetune.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validaion Loss')
plt.show()

# Setelah model selesai dilatih
train_loss, train_acc = model.evaluate(train_generator)
test_loss, test_acc = model.evaluate(test_generator)

print(f"Akurasi Training: {train_acc * 100:.2f}%")
print(f"Akurasi Testing: {test_acc * 100:.2f}%")

# Setelah model selesai dilatih dengan transfer learning MobileNetV2 (finetune)
train_loss, train_acc = model.evaluate(train_generator)
test_loss, test_acc = model.evaluate(test_generator)

print(f"Akurasi Training: {train_acc * 100:.2f}%")
print(f"Akurasi Testing: {test_acc * 100:.2f}%")

# Function to load and preprocess images
def load_and_preprocess_image(image_path):
    img = image.load_img(image_path, target_size=(224, 224))  # Load image with target size
    img = image.img_to_array(img)  # Convert to NumPy array
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    img = img / 255.0  # Normalize pixel values
    return img

# Load and preprocess all test images
test_images_processed = []
# Update this line to use the directory path
for class_name in os.listdir(test_dir):
    class_dir = os.path.join(test_dir, class_name)
    for image_name in os.listdir(class_dir):
        image_path = os.path.join(class_dir, image_name)
        test_images_processed.append(load_and_preprocess_image(image_path))

# Stack images into a single NumPy array
test_images_processed = np.vstack(test_images_processed)

# Now you can make predictions
y_pred = model.predict(test_images_processed)

# Get true labels from test generator
y_true = test_generator.classes

# Convert predictions to class labels (0 or 1)
y_pred_classes = (y_pred > 0.5).astype(int)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Buat confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Buat confusion matrix (MobileNetV2 Fine Tuning)
cm = confusion_matrix(y_true, y_pred_classes)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Konversi Model"""

save_path = 'saved_model/'
tf.saved_model.save(model, save_path)

model.save("model_drowsiness.h5")

converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
tflite_model = converter.convert()

with tf.io.gfile.GFile('model_drowsiness.tflite', 'wb') as f:
    f.write(tflite_model)

!pip install tensorflowjs

!tensorflowjs_converter \
    --input_format=tf_saved_model \
    /content/saved_model/ \
    model_tfjs

!zip -r model_tfjs.zip model_tfjs

!zip -r saved_model.zip saved_model/

"""## Inference"""

# Load model
model = tf.keras.models.load_model('model_drowsiness.h5')
print("Model loaded successfully!")

def preprocess_image(img_path):
    # Load gambar dengan ukuran 224x224 (ukuran input MobileNetV2)
    img = image.load_img(img_path, target_size=(224, 224))
    # Ubah gambar menjadi array
    img_array = image.img_to_array(img) / 255.0
    # Tambahkan dimensi batch
    img_array = np.expand_dims(img_array, axis=0)
    return img_array

def predict_eye_state(img_path):
    # Preprocessing gambar
    img_array = preprocess_image(img_path)

    # Prediksi menggunakan model
    prediction = model.predict(img_array)

    # Interpretasi hasil
    if prediction[0][0] > 0.5:
        result = "Mata Terbuka"
    else:
        result = "Mata Tertutup"

    # Visualisasi gambar
    plt.imshow(image.load_img(img_path))
    plt.title(f"Prediction: {result}")
    plt.axis('off')
    plt.show()

    return result

image_path = '/content/contoh-closed.jpg'
result = predict_eye_state(image_path)
print(f"Hasil Prediksi: {result}")

image_path = '/content/contoh-opened.jpg'
result = predict_eye_state(image_path)
print(f"Hasil Prediksi: {result}")